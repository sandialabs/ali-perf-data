{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALI Performance Tests on Blake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Currently testing the Greenland Ice Sheet (GIS) and Antartica Ice Sheet (AIS) in Albany Land Ice (ALI) using Intel Skylake CPUs on blake.\n",
    "\n",
    "### Architectures: \n",
    "| Name  | Blake (SKX) |\n",
    "|---|---|\n",
    "| CPU  | Dual-socket Intel<br/>Xeon Platinum 8160<br/>(Skylake) |\n",
    "| Cores/Node  | 48  |\n",
    "| Threads/Core  | 2 |\n",
    "| Memory/Node  | 187 GB |\n",
    "| Interconnect  | Intel OmniPath<br/>Gen-1 (100 GB/s)  |\n",
    "| Compiler  | Intel 18.1.163  |\n",
    "| MPI  | openmpi 2.1.2  |\n",
    "\n",
    "### Cases: \n",
    "| Case Name  | Number of Processes (np) | Description |\n",
    "|---|---|---|\n",
    "| ant-2-20km_ml_ls | 384 | Unstructured 2-20km AIS, ML w/ line smoothing |\n",
    "| ant-2-20km_mu_ls | 384 | Unstructured 2-20km AIS, MueLu w/ line smoothing |\n",
    "| ant-2-20km_mu_dls | 384 | Unstructured 2-20km AIS, MueLu w/ decoupled line smoothing |\n",
    "| green-1-10km_ent_fea_1ws_tet | 384 | Unstructured 1-10km GIS, Enthalpy problem, finite element assembly only, single workset, tetrahedron |\n",
    "| green-1-10km_ent_fea_1ws_wdg | 384 | Unstructured 1-10km GIS, Enthalpy problem, finite element assembly only, single workset, wedge |\n",
    "| green-1-10km_ent_fea_mem_tet | 384 | Unstructured 1-10km GIS, Enthalpy problem, finite element assembly only, memoization, tetrahedron |\n",
    "| green-1-10km_ent_fea_mem_wdg | 384 | Unstructured 1-10km GIS, Enthalpy problem, finite element assembly only, memoization, wedge |\n",
    "| green-1-10km_ent_mu_wdg | 384 | Unstructured 1-10km GIS, Enthalpy problem, MueLu, wedge |\n",
    "| green-1-7km_fea_1ws | 384 | Unstructured 1-7km GIS, finite element assembly only, single workset |\n",
    "| green-1-7km_mu_dls_1ws | 384 | Unstructured 1-7km GIS, MueLu w/ decoupled line smoothing, single workset |\n",
    "| green-1-7km_fea_mem | 384 | Unstructured 1-7km GIS, finite element assembly only, memoization |\n",
    "| green-1-7km_ml_ls_mem | 384 | Unstructured 1-7km GIS, ML w/ line smoothing, memoization |\n",
    "| green-1-7km_mu_ls_mem | 384 | Unstructured 1-7km GIS, MueLu w/ line smoothing, memoization |\n",
    "| green-1-7km_mu_dls_mem | 384 | Unstructured 1-7km GIS, MueLu w/ decoupled line smoothing, memoization |\n",
    "| green-1-7km_muk_ls_mem | 384 | Unstructured 1-7km GIS, MueLu w/ kokkos and line smoothing, memoization |\n",
    "| green-3-20km_vel_fea_mem_tet | 384 | Unstructured 3-20km GIS, Velocity problem, finite element assembly only, memoization, tetrahedron |\n",
    "| green-3-20km_vel_fea_mem_wdg | 384 | Unstructured 3-20km GIS, Velocity problem, finite element assembly only, memoization, wedge |\n",
    "| green-3-20km_ent_fea_mem_tet | 384 | Unstructured 3-20km GIS, Enthalpy problem, finite element assembly only, memoization, tetrahedron |\n",
    "| green-3-20km_ent_fea_mem_wdg | 384 | Unstructured 3-20km GIS, Enthalpy problem, finite element assembly only, memoization, wedge |\n",
    "| green-3-20km_beta_1ws | 384 | Unstructured 3-20km GIS, basal friction initialization, single workset |\n",
    "| green-3-20km_beta_mem | 384 | Unstructured 3-20km GIS, basal friction initialization, memoization |\n",
    "| green-3-20km_beta_memp | 384 | Unstructured 3-20km GIS, basal friction initialization, memoization for parameters (beta) |\n",
    "\n",
    "### Timers: \n",
    "| Timer Name | Level | Description |\n",
    "|---|---|---|\n",
    "| Albany Total Time | 0 | Total wall-clock time of simulation |\n",
    "| Albany: Setup Time | 1 | Preprocess |\n",
    "| Albany: Total Fill Time | 1 | Finite element assembly |\n",
    "| Albany Fill: Residual | 2 | Residual assembly |\n",
    "| Albany Residual Fill: Evaluate | 3 | Compute the residual, local/global assembly |\n",
    "| Albany Residual Fill: Export | 3 | Update global residual across MPI ranks |\n",
    "| Albany Fill: Jacobian | 2 | Jacobian assembly |\n",
    "| Albany Jacobian Fill: Evaluate | 3 | Compute the Jacobian, local/global assembly |\n",
    "| Albany Jacobian Fill: Export | 3 | Update global Jacobian across MPI ranks |\n",
    "| NOX Total Preconditioner Construction | 1 | Construct Preconditioner |\n",
    "| NOX Total Linear Solve | 1 | Linear Solve |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import multiprocessing\n",
    "import sys\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "\n",
    "# Import scripts\n",
    "sys.path.insert(0,'kcshan-perf-analysis')\n",
    "from json2timeline import json2dataframe\n",
    "from models import single_ts_chgpts\n",
    "from basicstats import add_regime_stats\n",
    "from basicstats import trimmed_stats\n",
    "from utils import *\n",
    "from scipy.stats import t as tdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Enable offline plot export\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration file\n",
    "with open('config.json') as jf:\n",
    "    config = json.load(jf)\n",
    "check_config(config)\n",
    "for key,val in config.items():\n",
    "        exec(key + '=val')\n",
    "\n",
    "# Extract file names and collect data\n",
    "files = glob.glob(json_regex)\n",
    "df = json2dataframe(files, cases, names, timers, metadata)\n",
    "if len(df) == 0:\n",
    "    raise RuntimeError('No data found; check json directory')\n",
    "\n",
    "# Log-transform the data before modeling\n",
    "xform = lambda x: np.log(x)\n",
    "inv_xform = lambda x: np.exp(x)\n",
    "\n",
    "# Add other metrics to name list\n",
    "names.append('max host memory')\n",
    "names.append('max kokkos memory')\n",
    "metadata.remove('max host memory')\n",
    "metadata.remove('max kokkos memory')\n",
    "\n",
    "# Filter data by date if desired\n",
    "import datetime as dt\n",
    "#df = df[df['date'] < dt.datetime.strptime('20211201', '%Y%m%d')]\n",
    "df = df[df['date'] > dt.datetime.strptime('20220205', '%Y%m%d')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test cases:')\n",
    "[print('  '+c) for c in cases]\n",
    "print('Metrics:')\n",
    "[print('  '+n) for n in names]\n",
    "print('Metadata:')\n",
    "[print('  '+m) for m in metadata]\n",
    "print(\"Model threshold: %f\" % threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Timelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.seterr(all='raise')\n",
    "\n",
    "def regime_diff_ts(y, changePts, std_error=False, alpha=0.01):\n",
    "    '''\n",
    "    Given a timeseries y and a set of changepoints, return three timeseries with the\n",
    "    same length as y, containing the mean difference and upper/lower bounds between changepoints\n",
    "    Inputs:\n",
    "        y        : time series input\n",
    "        changePts: sorted list of indices of y which are changepoints, \n",
    "                     where changePts[0] = 0\n",
    "        std_error: whether reported std should be std error of the mean\n",
    "        alpha    : if std_error, then this controls the significance level of the bounds\n",
    "    '''\n",
    "    y = make_numpy(y)\n",
    "    n = len(y)\n",
    "    m = len(changePts)\n",
    "    meandiff = np.zeros_like(y)\n",
    "    upper = np.zeros_like(y)\n",
    "    lower = np.zeros_like(y)\n",
    "    for i in range(1,m):\n",
    "        a = changePts[i-1]\n",
    "        b = changePts[i]\n",
    "        c = changePts[i+1] if i < m-1 else n\n",
    "        \n",
    "        y1mean, y1var = trimmed_stats(y[a:b], var=True)\n",
    "        n1 = len(y[a:b])\n",
    "        y2mean, y2var = trimmed_stats(y[b:c], var=True)\n",
    "        n2 = len(y[b:c])\n",
    "        meandiff[b:c] = np.abs(y1mean-y2mean)\n",
    "        if (n1 == 1 and n2 == 1):\n",
    "            continue\n",
    "        \n",
    "        rss = (n1-1)*y1var + (n2-1)*y2var\n",
    "        std = np.sqrt(rss/(n1+n2-2)) + 1e-8\n",
    "        if std_error:\n",
    "            std /= np.sqrt(n1*n2/(n1+n2))\n",
    "            tcrit = tdist.isf(alpha/2, df=n1+n2-2)\n",
    "        else:\n",
    "            tcrit = 2\n",
    "        upper[b:c] = meandiff[b:c] + tcrit*std\n",
    "        lower[b:c] = meandiff[b:c] - tcrit*std\n",
    "    return meandiff, upper, lower\n",
    "\n",
    "def add_regime_diff_stats(df, changePts, std_error=False, alpha=0.01):\n",
    "    '''\n",
    "    Take a dataframe with a 'time' column, and append the output of regime_diff_ts\n",
    "    '''\n",
    "    meandiff, upper, lower = regime_diff_ts(df['time'], changePts, std_error, alpha)\n",
    "    temp = {'meandiff': meandiff, 'meandiff_upper': upper, 'meandiff_lower': lower}\n",
    "    return pd.concat((df, pd.DataFrame(temp)), axis=1)\n",
    "\n",
    "\n",
    "# Find changepoints and format data to work nicely with plots\n",
    "seqs = {case:{} for case in cases}\n",
    "most_recent = df['date'].max()\n",
    "events = {}\n",
    "pool = multiprocessing.Pool(4)\n",
    "\n",
    "print('Finding changepoints:')\n",
    "for case in cases:\n",
    "    print(case)\n",
    "    \n",
    "    # Add time data to seqs\n",
    "    for name in names:\n",
    "        cols = ['date', name] + list(metadata)\n",
    "        data = df.loc[df['case']==case, cols].dropna(subset=[name])\n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "        data.rename(columns={name:'time'}, inplace=True)\n",
    "        data['time'] = xform(data['time'])\n",
    "        seqs[case][name] = data\n",
    "    \n",
    "    # Detect changepoints  \n",
    "    pool_inputs = [(k, v, threshold) for k,v in seqs[case].items()]\n",
    "    chgpts = dict(pool.map(single_ts_chgpts, pool_inputs))\n",
    "    \n",
    "    for name in names:\n",
    "        # Calculate mean/99%CI between changepoints\n",
    "        seqs[case][name] = add_regime_stats(seqs[case][name], chgpts[name], std_error=True, alpha=0.01)\n",
    "        seqs[case][name]['means'] = inv_xform(seqs[case][name].pop('mean'))\n",
    "        seqs[case][name]['means_lower'] = np.nan_to_num(inv_xform(seqs[case][name].pop('lower')))\n",
    "        seqs[case][name]['means_upper'] = np.nan_to_num(inv_xform(seqs[case][name].pop('upper')))\n",
    "        \n",
    "        # Calculate mean/std between changepoints\n",
    "        seqs[case][name] = add_regime_stats(seqs[case][name], chgpts[name])\n",
    "        \n",
    "        # Calculate meandiff/99%CI between changepoints\n",
    "        seqs[case][name] = add_regime_diff_stats(seqs[case][name], chgpts[name], std_error=True, alpha=0.01)\n",
    "        seqs[case][name]['meandiff'] = inv_xform(seqs[case][name]['meandiff'])\n",
    "        seqs[case][name]['meandiff_lower'] = inv_xform(seqs[case][name]['meandiff_lower'])\n",
    "        seqs[case][name]['meandiff_upper'] = inv_xform(seqs[case][name]['meandiff_upper'])\n",
    "        \n",
    "        # Build dictionary of changepoints\n",
    "        for d in seqs[case][name]['date'].iloc[chgpts[name]]:\n",
    "            events.setdefault(d, {}).setdefault(case, []).append(name)\n",
    "clear_output()\n",
    "\n",
    "# Sort and print results\n",
    "events = {k:events[k] for k in sorted(events.keys())}\n",
    "print('Events in the most recent %d days:' % recency)\n",
    "recent_events = print_events(events, most_recent, recency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lines = ['time', 'mean', 'upper', 'lower']\n",
    "colors = ['blue', 'red', 'red', 'red']\n",
    "modes = ['markers', 'lines', 'lines', 'lines']\n",
    "dashes = ['solid', 'solid', 'dot', 'dot']\n",
    "\n",
    "fig = go.Figure()\n",
    "# Create series on plot\n",
    "for line, color, mode, dash in zip(lines, colors, modes, dashes):\n",
    "    for c in cases:\n",
    "        if line == 'time':\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=seqs[c][names[0]]['date'],\n",
    "                y=inv_xform(seqs[c][names[0]][line]),\n",
    "                mode=mode,\n",
    "                line = dict(color=color, dash=dash, width=1.5),\n",
    "                name=line,\n",
    "                visible=True if c==cases[0] else False,\n",
    "                customdata=seqs[c][names[0]][['date']+['means']+['means_lower']+['means_upper']+['meandiff']+['meandiff_lower']+['meandiff_upper']+list(metadata)],\n",
    "                hovertemplate=\n",
    "                \"Date: %{customdata[0]}<br>\" +\n",
    "                \"Albany commit: %{customdata[8]}<br>\" +\n",
    "                \"Trilinos commit: %{customdata[9]}<br>\" +\n",
    "                \"Mean (99% CI): %{customdata[1]:.3g} (%{customdata[2]:.3g}, %{customdata[3]:.3g})<br>\" +\n",
    "                \"Ratio (99% CI): %{customdata[4]:1.2f} (%{customdata[5]:1.2f}, %{customdata[6]:1.2f})\" +\n",
    "                \"<extra></extra>\",\n",
    "            ))\n",
    "        else:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=seqs[c][names[0]]['date'],\n",
    "                y=inv_xform(seqs[c][names[0]][line]),\n",
    "                mode=mode,\n",
    "                line = dict(color=color, dash=dash, width=1.5),\n",
    "                name=line,\n",
    "                visible=True if c==cases[0] else False,\n",
    "                hoverinfo='skip'\n",
    "            ))\n",
    "\n",
    "changed_cases = {n for v in recent_events.values() for n in v.keys()}\n",
    "\n",
    "# Test case dropdown\n",
    "case_options = [dict(\n",
    "        args=['visible', [True if x==c else False for x in np.tile(cases, len(lines))]],\n",
    "        label= '*'+c if c in changed_cases else c,\n",
    "        method='restyle'\n",
    "    ) for c in cases]\n",
    "    \n",
    "# Timer dropdown\n",
    "name_options = [dict(\n",
    "        args=[{'x': [seqs[c][n]['date'] for _ in lines for c in cases],\n",
    "               'y': [inv_xform(seqs[c][n][line]) for line in lines for c in cases],\n",
    "               'customdata': [seqs[c][n][['date']+['means']+['means_lower']+['means_upper']+['meandiff']+['meandiff_lower']+['meandiff_upper']+list(metadata)].to_numpy()\n",
    "                              if line == 'time' else None\n",
    "                              for line in lines for c in cases]}],\n",
    "        label=n,\n",
    "        method='restyle'\n",
    "    ) for n in names]\n",
    "\n",
    "# Add dropdowns to plot\n",
    "fig.update_layout(\n",
    "    updatemenus=[\n",
    "        go.layout.Updatemenu(\n",
    "            buttons=list(case_options),\n",
    "            direction=\"down\",\n",
    "            pad={\"r\": 10, \"t\": 10},\n",
    "            showactive=True,\n",
    "            x=0,    xanchor=\"left\",\n",
    "            y=1.15, yanchor=\"top\"\n",
    "        ),\n",
    "        go.layout.Updatemenu(\n",
    "            buttons=list(name_options),\n",
    "            direction=\"down\",\n",
    "            pad={\"r\": 10, \"t\": 10},\n",
    "            showactive=True,\n",
    "            x=0.3, xanchor=\"left\",\n",
    "            y=1.15, yanchor=\"top\"\n",
    "        ),\n",
    "    ],\n",
    "    margin={'l': 50, 'r': 50, 'b': 200, 't': 50},\n",
    "    height=600,\n",
    "    xaxis_title='Simulation Date',\n",
    "    yaxis_title='Wall-clock Time (s) or Memory (MiB)'\n",
    ")\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of wall-clock times or memory for nightly runs\n",
    "Changepoints are estimated using a generalized likelihood ratio method on each timer, and then merged over all timers for a given test case. \n",
    "* Blue markers: recorded wall-clock time or memory\n",
    "* Solid red line: average wall-clock time or memory between changepoints\n",
    "* Dotted red lines: average wall-clock time or memory $\\pm$ two standard deviations\n",
    "\n",
    "#### Plot window controls\n",
    "\n",
    "* Test case and timer can be selected from the drop-down menus (* denotes recent changes detected)\n",
    "* Hovering over data points shows various metadata.\n",
    "  * The mean with upper and lower bounds of a 99% confidence interval.\n",
    "  * The relative performance (speedup,slowdown) between sets identified by changepoints with a 99% confidence interval for the ratio (difference between log mean of the two sets).\n",
    "  * Note: The confidence interval is based on a t-statistic, so for very small amounts of data (<5), the interval may be very large. NaNs are converted to zero.\n",
    "* Clicking on the legend will show/hide individual plot elements\n",
    "* Click and drag to zoom in; double click to reset zoom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pollak, Moshe; Siegmund, D. Sequential Detection of a Change in a Normal Mean when the Initial Value is Unknown. Ann. Statist. 19 (1991), no. 1, 394--416. doi:10.1214/aos/1176347990. https://projecteuclid.org/euclid.aos/1176347990\n",
    "\n",
    "Siegmund, D.; Venkatraman, E. S. Using the Generalized Likelihood Ratio Statistic for Sequential Detection of a Change-Point. Ann. Statist. 23 (1995), no. 1, 255--271. doi:10.1214/aos/1176324466. https://projecteuclid.org/euclid.aos/1176324466\n",
    "\n",
    "Hawkins, D. M., & Zamba, K. D. (2005). Statistical Process Control for Shifts in Mean or Variance using a Change Point Formulation. Technometrics, 47, 164-173.\n",
    "\n",
    "Hawkins DM, Qiu P, Kang CW. The changepoint model for statistical process control. Journal of Quality Technology. 2003 Oct 1;35(4):355-366."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "number",
   "current_citInitial": 1,
   "eqLabelWithNumbers": false,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
